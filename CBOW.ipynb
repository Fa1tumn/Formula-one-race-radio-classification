{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76cae504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0278cf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Box now, box now for softs.', 'Copy that, coming in at the end of this lap.', 'Stay out, stay out, abort pit stop.', 'Pit confirm, pit confirm.', 'Box opposite to what Leclerc does this lap.', \"Let's undercut Hamilton, box now, box now.\", 'Safety car deployed, box now for softs.', 'Box this lap, box box box.', 'Box now, we need to react to Verstappen.', 'Box now for intermediates, rain is getting heavier.', \"We'll pit in two laps, prepare for a front wing change.\", 'Virtual safety car, box now, box now.', 'The undercut is powerful here, box this lap.', 'Staying out, staying out, this tire has more life.', 'Pit next lap for the prime tire, acknowledge.', \"Box when you feel it's right, your call on slicks.\", \"Box this lap, we need to cover Alonso's stop.\", 'Pit now for mediums, we have a gap in traffic.', 'Box box box, slow puncture suspected.', 'Rear tire pressures dropping, keep an eye on it.', 'Engine temperatures rising, we need to manage this.', 'Battery state is optimal, you can use overtake button.', 'Front left brake temperature critical, lift and coast into turn 1.', \"MGUK harvesting is limited, we're investigating.\", 'Loss of hydraulic pressure, use minimal steering input.', 'Oil temperature stable now, good job managing it.', 'Fuel flow is inconsistent, we may need to switch sensors.', \"DRS failure confirmed, we can't fix it during the race.\", 'Wind direction changed, expect more oversteer in turn 7.', 'Cooling is marginal but under control for now.', 'Water pressure dropping slightly, please minimize kerb usage.', 'ERS deployment optimized for this lap, full power available.', 'Front wing damage detected, but aero figures still acceptable.', 'Suspension is holding up after that bump, data looks okay.', 'Power unit temperature back in the window, good management.', 'Tire wear 10% higher than predicted, adjust your driving.', 'Brake balance shift recommended, try forward two clicks.', \"We've lost some downforce, possibly floor damage.\", 'Battery temperature rising, we need to manage energy deployment.', 'These blue flags are ridiculous!', 'The balance is terrible, so much understeer.', 'Tell him to give me more space! That was dangerous.', 'What are we doing? This strategy is not working!', 'Why did you bring me in? The tires were still good!', 'This traffic is costing us so much time!', \"He's blocking me! Ask race control to look at it.\", \"I can't follow in this dirty air, the car is overheating.\", \"The brakes are inconsistent, I can't trust them.\", \"This engine mode is too slow, I'm a sitting duck on the straights.\", \"What is the guy in front doing? He's all over the place!\", \"The rear is so nervous on corner entry, I can't push.\", 'These kerbs are destroying the floor, we need to avoid them.', \"I'm getting no traction out of turn 4, something's wrong.\", \"Can I get a drink? The button isn't working.\", 'Why are we on these tires? Everyone else is on softs!', \"There's so much porpoising down the straight, it's unbearable.\", 'Can we try something different with the strategy?', 'This penalty is unfair! I left enough space!', 'I need more information about the gap behind, please.', 'The gap to P5 is 1.2 seconds and closing.', 'Push now, we need 3 purple sectors.', 'We need to manage these tires to the end, target lap time 1:22.5.', 'Pit window is open, what do you think about the tires?', \"Stay out, Verstappen just pitted, let's overcut.\", \"We're going long on this stint, target plus 5 laps.\", \"Track position is critical here, we'll go to the end on these tires.\", 'Gap to P2 is stable at 4.3 seconds, manage the pace.', \"Blue flags for Alonso behind, don't lose time.\", \"Rain expected in 8 minutes, we're staying on slicks for now.\", 'DRS is enabled, use it on the main straight.', \"We're switching to Plan C, confirm you understand.\", 'Attack mode now, use all available battery.', 'Hold position, team orders, I repeat, hold position.', 'Fuel saving mode delta, target minus 0.2 per lap.', 'VSC ending, prepare to push.', 'We need to extend this stint by 2 laps to cover Perez.', \"The medium compound should last 25 laps, we're targeting 28.\", 'Use overtake button on the exit of turn 11 for best effect.', 'Defending position is priority, focus on exit speed.', 'Great job! That was a perfect lap!', \"Keep pushing, you're the fastest car on track right now.\", \"Brilliant overtake! Now let's catch the next one.\", \"You're doing a fantastic job managing these tires.\", \"That's exactly what we needed, perfect execution.\", \"Stay focused, you're driving beautifully today.\", 'We believe in you, just keep it clean to the finish.', \"Amazing pace in these conditions, you're making the difference.\", \"Outstanding first sector, you've found something there!\", \"Don't give up, we can still fight for points here.\", 'Your feedback is really helping us improve the car, thank you.', 'That defensive driving was textbook, well done.', \"You're extracting everything from the package, the team is proud.\", 'Keep your head down, this strategy will pay off in the end.', 'Perfect timing on that overtake, you made it look easy.', \"Trust your instincts, they're serving you well today.\", \"The whole team is behind you, let's bring this home.\", \"You're in the zone today, fantastic rhythm.\", \"The gap is coming down, you're reeling them in!\", \"Controlled, precise, and fast - that's what we like to see.\"]\n",
      "['진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '진입 명령', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '차량 상태', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '불만/요청', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '전략/전술', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려', '격려']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "#remove the first row because it is 'nan'\n",
    "texts = data[\"Message\"].tolist()[1:]\n",
    "labels = data[\"Category\"].tolist()[1:]\n",
    "print(texts)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0243d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [text.lower().split() for text in texts]\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "idx = 0\n",
    "for sentence in tokenized:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = idx\n",
    "            idx2word[idx] = word\n",
    "            idx += 1\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa2ac4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {\n",
    "    \"진입 명령\": 0,\n",
    "    \"차량 상태\": 1,\n",
    "    \"불만/요청\": 2,\n",
    "    \"전략/전술\": 3,\n",
    "    \"격려\": 4\n",
    "}\n",
    "idx2label = {v: k for k, v in label2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ec311ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for sentence, label in zip(tokenized, labels):\n",
    "    context_vec = np.zeros(vocab_size)\n",
    "    for word in sentence:\n",
    "        context_vec[word2idx[word]] += 1\n",
    "    X_data.append(context_vec / len(sentence))\n",
    "    y_data.append(label)\n",
    "\n",
    "X = np.array(X_data)\n",
    "y = [label2idx[label] for label in labels]\n",
    "num_classes = len(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21221b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "W_embed = np.random.randn(vocab_size, embedding_dim) * 0.01  # 단어 임베딩\n",
    "W_out = np.random.randn(embedding_dim, num_classes) * 0.01   # 출력 가중치\n",
    "b_out = np.zeros((1, num_classes))\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8bcaed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(probs, y_true):\n",
    "    batch_size = probs.shape[0]\n",
    "    log_probs = np.log(probs + 1e-9)\n",
    "    loss = -log_probs[range(batch_size), y_true].mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ae29f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(len(X) * (1 - test_size))\n",
    "    train_idx, test_idx = indices[:split], indices[split:]\n",
    "    return (\n",
    "        np.array([X[i] for i in train_idx]),  # Convert to NumPy array\n",
    "        np.array([X[i] for i in test_idx]),  # Convert to NumPy array\n",
    "        [y[i] for i in train_idx],\n",
    "        [y[i] for i in test_idx]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fb38954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 1.6059\n",
      "Epoch 10 | Loss: 1.6059\n",
      "Epoch 20 | Loss: 1.6059\n",
      "Epoch 30 | Loss: 1.6059\n",
      "Epoch 40 | Loss: 1.6058\n",
      "Epoch 50 | Loss: 1.6058\n",
      "Epoch 60 | Loss: 1.6058\n",
      "Epoch 70 | Loss: 1.6058\n",
      "Epoch 80 | Loss: 1.6058\n",
      "Epoch 90 | Loss: 1.6058\n",
      "Epoch 100 | Loss: 1.6058\n",
      "Epoch 110 | Loss: 1.6057\n",
      "Epoch 120 | Loss: 1.6057\n",
      "Epoch 130 | Loss: 1.6057\n",
      "Epoch 140 | Loss: 1.6057\n",
      "Epoch 150 | Loss: 1.6057\n",
      "Epoch 160 | Loss: 1.6057\n",
      "Epoch 170 | Loss: 1.6056\n",
      "Epoch 180 | Loss: 1.6056\n",
      "Epoch 190 | Loss: 1.6056\n",
      "Epoch 200 | Loss: 1.6056\n",
      "Epoch 210 | Loss: 1.6055\n",
      "Epoch 220 | Loss: 1.6055\n",
      "Epoch 230 | Loss: 1.6055\n",
      "Epoch 240 | Loss: 1.6055\n",
      "Epoch 250 | Loss: 1.6054\n",
      "Epoch 260 | Loss: 1.6054\n",
      "Epoch 270 | Loss: 1.6053\n",
      "Epoch 280 | Loss: 1.6053\n",
      "Epoch 290 | Loss: 1.6053\n",
      "Epoch 300 | Loss: 1.6052\n",
      "Epoch 310 | Loss: 1.6051\n",
      "Epoch 320 | Loss: 1.6051\n",
      "Epoch 330 | Loss: 1.6050\n",
      "Epoch 340 | Loss: 1.6050\n",
      "Epoch 350 | Loss: 1.6049\n",
      "Epoch 360 | Loss: 1.6048\n",
      "Epoch 370 | Loss: 1.6047\n",
      "Epoch 380 | Loss: 1.6047\n",
      "Epoch 390 | Loss: 1.6046\n",
      "Epoch 400 | Loss: 1.6045\n",
      "Epoch 410 | Loss: 1.6044\n",
      "Epoch 420 | Loss: 1.6043\n",
      "Epoch 430 | Loss: 1.6041\n",
      "Epoch 440 | Loss: 1.6040\n",
      "Epoch 450 | Loss: 1.6039\n",
      "Epoch 460 | Loss: 1.6037\n",
      "Epoch 470 | Loss: 1.6036\n",
      "Epoch 480 | Loss: 1.6034\n",
      "Epoch 490 | Loss: 1.6032\n",
      "Epoch 500 | Loss: 1.6030\n",
      "Epoch 510 | Loss: 1.6028\n",
      "Epoch 520 | Loss: 1.6026\n",
      "Epoch 530 | Loss: 1.6023\n",
      "Epoch 540 | Loss: 1.6021\n",
      "Epoch 550 | Loss: 1.6018\n",
      "Epoch 560 | Loss: 1.6015\n",
      "Epoch 570 | Loss: 1.6012\n",
      "Epoch 580 | Loss: 1.6008\n",
      "Epoch 590 | Loss: 1.6004\n",
      "Epoch 600 | Loss: 1.6000\n",
      "Epoch 610 | Loss: 1.5996\n",
      "Epoch 620 | Loss: 1.5991\n",
      "Epoch 630 | Loss: 1.5986\n",
      "Epoch 640 | Loss: 1.5981\n",
      "Epoch 650 | Loss: 1.5975\n",
      "Epoch 660 | Loss: 1.5969\n",
      "Epoch 670 | Loss: 1.5963\n",
      "Epoch 680 | Loss: 1.5956\n",
      "Epoch 690 | Loss: 1.5948\n",
      "Epoch 700 | Loss: 1.5940\n",
      "Epoch 710 | Loss: 1.5931\n",
      "Epoch 720 | Loss: 1.5922\n",
      "Epoch 730 | Loss: 1.5912\n",
      "Epoch 740 | Loss: 1.5902\n",
      "Epoch 750 | Loss: 1.5890\n",
      "Epoch 760 | Loss: 1.5878\n",
      "Epoch 770 | Loss: 1.5865\n",
      "Epoch 780 | Loss: 1.5851\n",
      "Epoch 790 | Loss: 1.5836\n",
      "Epoch 800 | Loss: 1.5820\n",
      "Epoch 810 | Loss: 1.5803\n",
      "Epoch 820 | Loss: 1.5785\n",
      "Epoch 830 | Loss: 1.5766\n",
      "Epoch 840 | Loss: 1.5745\n",
      "Epoch 850 | Loss: 1.5723\n",
      "Epoch 860 | Loss: 1.5700\n",
      "Epoch 870 | Loss: 1.5675\n",
      "Epoch 880 | Loss: 1.5649\n",
      "Epoch 890 | Loss: 1.5621\n",
      "Epoch 900 | Loss: 1.5592\n",
      "Epoch 910 | Loss: 1.5560\n",
      "Epoch 920 | Loss: 1.5527\n",
      "Epoch 930 | Loss: 1.5493\n",
      "Epoch 940 | Loss: 1.5456\n",
      "Epoch 950 | Loss: 1.5417\n",
      "Epoch 960 | Loss: 1.5377\n",
      "Epoch 970 | Loss: 1.5334\n",
      "Epoch 980 | Loss: 1.5290\n",
      "Epoch 990 | Loss: 1.5243\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1) 임베딩 평균화\n",
    "    X_embed = np.dot(X_train, W_embed)  # shape: (batch_size, embedding_dim)\n",
    "\n",
    "    # 2) 출력 계산\n",
    "    logits = np.dot(X_embed, W_out) + b_out\n",
    "    probs = softmax(logits)\n",
    "\n",
    "    # 3) 손실\n",
    "    loss = cross_entropy(probs, y_train)\n",
    "\n",
    "    # 4) 역전파\n",
    "    N = X_train.shape[0]\n",
    "    one_hot = np.zeros_like(probs)\n",
    "    one_hot[np.arange(N), y_train] = 1\n",
    "    dL_dz = (probs - one_hot) / N\n",
    "\n",
    "    dW_out = np.dot(X_embed.T, dL_dz)\n",
    "    db_out = np.sum(dL_dz, axis=0, keepdims=True)\n",
    "\n",
    "    dX_embed = np.dot(dL_dz, W_out.T)  # shape: (N, embedding_dim)\n",
    "    dW_embed = np.dot(X_train.T, dX_embed)  # shape: (vocab_size, embedding_dim)\n",
    "\n",
    "    # 5) 파라미터 업데이트\n",
    "    W_out -= learning_rate * dW_out\n",
    "    b_out -= learning_rate * db_out\n",
    "    W_embed -= learning_rate * dW_embed\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3c3d8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 정확도: 0.1\n"
     ]
    }
   ],
   "source": [
    "test_logits = np.dot(X_test, W_embed) @ W_out + b_out  # Forward pass\n",
    "test_probs = softmax(test_logits)  # Softmax to get probabilities\n",
    "preds = np.argmax(test_probs, axis=1)  # Predicted class\n",
    "acc = np.mean(preds == y_test)  # Accuracy calculation\n",
    "\n",
    "print(\"\\n테스트 정확도:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c1c226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"Push harder now\" | Predicted Label: \"불만/요청\"\n",
      "Text: \"Engine is failing\" | Predicted Label: \"불만/요청\"\n"
     ]
    }
   ],
   "source": [
    "# new data 예측\n",
    "new_texts = [\n",
    "    \"Push harder now\", \n",
    "    \"Engine is failing\"\n",
    "]\n",
    "\n",
    "# 1. 分词并向量化\n",
    "new_tokenized = [text.lower().split() for text in new_texts]\n",
    "new_X_data = []\n",
    "\n",
    "for sentence in new_tokenized:\n",
    "    context_vec = np.zeros(vocab_size)\n",
    "    for word in sentence:\n",
    "        if word in word2idx:  # if in vocabulary\n",
    "            context_vec[word2idx[word]] += 1\n",
    "    new_X_data.append(context_vec / len(sentence))\n",
    "\n",
    "new_X = np.array(new_X_data)\n",
    "\n",
    "new_X_embed = np.dot(new_X, W_embed)\n",
    "\n",
    "new_logits = np.dot(new_X_embed, W_out) + b_out\n",
    "new_probs = softmax(new_logits)\n",
    "\n",
    "new_y_pred = np.argmax(new_probs, axis=1)\n",
    "\n",
    "new_y_pred_labels = [idx2label[idx] for idx in new_y_pred]\n",
    "\n",
    "for text, label in zip(new_texts, new_y_pred_labels):\n",
    "    print(f\"Text: \\\"{text}\\\" | Predicted Label: \\\"{label}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e775060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_data = {\n",
    "    \"W_embed\": W_embed,\n",
    "    \"W_out\": W_out,\n",
    "    \"b_out\": b_out,\n",
    "    \"word2idx\": word2idx,\n",
    "    \"idx2label\": idx2label,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"num_classes\": num_classes\n",
    "}\n",
    "\n",
    "with open(\"cbow_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
