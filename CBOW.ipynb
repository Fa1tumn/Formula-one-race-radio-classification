{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "76cae504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0278cf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Make a gap so you can weave on this straight and then we’ll have a lift and brake into 11.', 'We’ll need a 1.50 lift into turn 10.', 'Brake into 8. Easy through 8.', 'No full pedal from now.', 'I have understeer at turn 80 and oversteer at turn 80.', 'Everything is overheating.', 'I don’t like the feedback on the start. Yeah, it’s too deep.', 'Front’s still too cold. Just had a test.', 'This tyre sucks.', 'We are boxing for hard tyres.', 'Keep working those tyres.', 'Focusing on putting temperature on these tyres.', 'Medium tyres through 11 and 12.', 'Our tyres are on mode 3A.', 'box box box.', 'Plan 12. Plus 0.5.', 'We are plan B, target lap, both safety car windows are open.', 'We’ll box this lap.', 'Next tyres will be a hard ollie.', 'You have clear, clear now. The car ahead is Stroll in turn 2.', 'Wind is reducing. Wind is reduced 5 kph.', 'Everyone slow ahead.', 'Lando is over his grid box.', 'Isaac has DRS behind.', 'WooHoo, let’s go baby! Love that.', 'I am stupid. I switching off everything.', 'Simple as that. What do you think? Do you think that was a Smooth operator~', 'Yeah leave me alone, Ricky.', 'What a race, P13 to P3. That’s why you should never give up.', 'There is a chance we lose the dash, if that happens just keep driving.', 'Gears are ****, what’s going on?', 'It felt all under control, and then we had a brake by wire failure.', 'Gears are ****, what’s going on?', 'Gears are ****, what’s going on?', 'Turn 10, good improvement. Turn 4, we’re losing 40 milliseconds compared to Lewis.', 'Comparator on target paces and cars ahead, 37. med 37.', 'Turn 10, good improvement. Turn 4, we’re losing 40 milliseconds compared to Lewis.', 'Turn 10, good improvement. Turn 4, we’re losing 40 milliseconds compared to Lewis.', 'Turn 10, good improvement. Turn 4, we’re losing 40 milliseconds compared to Lewis.']\n",
      "['차량 조작 지시', '차량 조작 지시', '차량 조작 지시', '차량 조작 지시', '차량 상태 피드백', '차량 상태 피드백', '차량 상태 피드백', '차량 상태 피드백', '차량 상태 피드백', '타이어 관련', '타이어 관련', '타이어 관련', '타이어 관련', '타이어 관련', '전략 및 계획', '전략 및 계획', '전략 및 계획', '전략 및 계획', '전략 및 계획', '트랙/상대 정보', '트랙/상대 정보', '트랙/상대 정보', '트랙/상대 정보', '트랙/상대 정보', '감성/개인 표현', '감성/개인 표현', '감성/개인 표현', '감성/개인 표현', '감성/개인 표현', '시스템/하드웨어 문제', '시스템/하드웨어 문제', '시스템/하드웨어 문제', '시스템/하드웨어 문제', '시스템/하드웨어 문제', '데이터 피드백/비교', '데이터 피드백/비교', '데이터 피드백/비교', '데이터 피드백/비교', '데이터 피드백/비교']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "#remove the first row because it is 'nan'\n",
    "texts = data[\"Message\"].tolist()[1:]\n",
    "labels = data[\"Category\"].tolist()[1:]\n",
    "print(texts)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0243d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [text.lower().split() for text in texts]\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "idx = 0\n",
    "for sentence in tokenized:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = idx\n",
    "            idx2word[idx] = word\n",
    "            idx += 1\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fa2ac4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {\n",
    "    \"차량 조작 지시\": 0,\n",
    "    \"차량 상태 피드백\": 1,\n",
    "    \"타이어 관련\": 2,\n",
    "    \"전략 및 계획\": 3,\n",
    "    \"트랙/상대 정보\": 4,\n",
    "    \"감성/개인 표현\": 5,\n",
    "    \"시스템/하드웨어 문제\": 6,\n",
    "    \"데이터 피드백/비교\": 7\n",
    "}\n",
    "idx2label = {v: k for k, v in label2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9ec311ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for sentence, label in zip(tokenized, labels):\n",
    "    context_vec = np.zeros(vocab_size)\n",
    "    for word in sentence:\n",
    "        context_vec[word2idx[word]] += 1\n",
    "    X_data.append(context_vec / len(sentence))\n",
    "    y_data.append(label)\n",
    "\n",
    "X = np.array(X_data)\n",
    "y = [label2idx[label] for label in labels]\n",
    "num_classes = len(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "21221b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "W_embed = np.random.randn(vocab_size, embedding_dim) * 0.01  # 단어 임베딩\n",
    "W_out = np.random.randn(embedding_dim, num_classes) * 0.01   # 출력 가중치\n",
    "b_out = np.zeros((1, num_classes))\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f8bcaed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(probs, y_true):\n",
    "    batch_size = probs.shape[0]\n",
    "    log_probs = np.log(probs + 1e-9)\n",
    "    loss = -log_probs[range(batch_size), y_true].mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4ae29f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(len(X) * (1 - test_size))\n",
    "    train_idx, test_idx = indices[:split], indices[split:]\n",
    "    return (\n",
    "        np.array([X[i] for i in train_idx]),  # Convert to NumPy array\n",
    "        np.array([X[i] for i in test_idx]),  # Convert to NumPy array\n",
    "        [y[i] for i in train_idx],\n",
    "        [y[i] for i in test_idx]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8fb38954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 2.0795\n",
      "Epoch 10000 | Loss: 1.9983\n",
      "Epoch 20000 | Loss: 0.4427\n",
      "Epoch 30000 | Loss: 0.0626\n",
      "Epoch 40000 | Loss: 0.0255\n",
      "Epoch 50000 | Loss: 0.0149\n",
      "Epoch 60000 | Loss: 0.0103\n",
      "Epoch 70000 | Loss: 0.0077\n",
      "Epoch 80000 | Loss: 0.0061\n",
      "Epoch 90000 | Loss: 0.0050\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "epochs = 100000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1) 임베딩 평균화\n",
    "    X_embed = np.dot(X_train, W_embed)  # shape: (batch_size, embedding_dim)\n",
    "\n",
    "    # 2) 출력 계산\n",
    "    logits = np.dot(X_embed, W_out) + b_out\n",
    "    probs = softmax(logits)\n",
    "\n",
    "    # 3) 손실\n",
    "    loss = cross_entropy(probs, y_train)\n",
    "\n",
    "    # 4) 역전파\n",
    "    N = X_train.shape[0]\n",
    "    one_hot = np.zeros_like(probs)\n",
    "    one_hot[np.arange(N), y_train] = 1\n",
    "    dL_dz = (probs - one_hot) / N\n",
    "\n",
    "    dW_out = np.dot(X_embed.T, dL_dz)\n",
    "    db_out = np.sum(dL_dz, axis=0, keepdims=True)\n",
    "\n",
    "    dX_embed = np.dot(dL_dz, W_out.T)  # shape: (N, embedding_dim)\n",
    "    dW_embed = np.dot(X_train.T, dX_embed)  # shape: (vocab_size, embedding_dim)\n",
    "\n",
    "    # 5) 파라미터 업데이트\n",
    "    W_out -= learning_rate * dW_out\n",
    "    b_out -= learning_rate * db_out\n",
    "    W_embed -= learning_rate * dW_embed\n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f3c3d8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 정확도: 0.5\n"
     ]
    }
   ],
   "source": [
    "test_logits = np.dot(X_test, W_embed) @ W_out + b_out  # Forward pass\n",
    "test_probs = softmax(test_logits)  # Softmax to get probabilities\n",
    "preds = np.argmax(test_probs, axis=1)  # Predicted class\n",
    "acc = np.mean(preds == y_test)  # Accuracy calculation\n",
    "\n",
    "print(\"\\n테스트 정확도:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6c1c226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"box box box\" | Predicted Label: \"전략 및 계획\"\n",
      "Text: \"My tyres are gone.\" | Predicted Label: \"타이어 관련\"\n",
      "Text: \"Tell him to get out of the way.\" | Predicted Label: \"차량 상태 피드백\"\n",
      "Text: \"We need to push for track position.\" | Predicted Label: \"타이어 관련\"\n"
     ]
    }
   ],
   "source": [
    "# new data 예측\n",
    "new_texts = [\n",
    "    \"box box box\", \n",
    "    \"My tyres are gone.\",\n",
    "    \"Tell him to get out of the way.\",\n",
    "    \"We need to push for track position.\",\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "# 1. 分词并向量化\n",
    "new_tokenized = [text.lower().split() for text in new_texts]\n",
    "new_X_data = []\n",
    "\n",
    "for sentence in new_tokenized:\n",
    "    context_vec = np.zeros(vocab_size)\n",
    "    for word in sentence:\n",
    "        if word in word2idx:  # if in vocabulary\n",
    "            context_vec[word2idx[word]] += 1\n",
    "    new_X_data.append(context_vec / len(sentence))\n",
    "\n",
    "new_X = np.array(new_X_data)\n",
    "\n",
    "new_X_embed = np.dot(new_X, W_embed)\n",
    "\n",
    "new_logits = np.dot(new_X_embed, W_out) + b_out\n",
    "new_probs = softmax(new_logits)\n",
    "\n",
    "new_y_pred = np.argmax(new_probs, axis=1)\n",
    "\n",
    "new_y_pred_labels = [idx2label[idx] for idx in new_y_pred]\n",
    "\n",
    "for text, label in zip(new_texts, new_y_pred_labels):\n",
    "    print(f\"Text: \\\"{text}\\\" | Predicted Label: \\\"{label}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3e775060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_data = {\n",
    "    \"W_embed\": W_embed,\n",
    "    \"W_out\": W_out,\n",
    "    \"b_out\": b_out,\n",
    "    \"word2idx\": word2idx,\n",
    "    \"idx2label\": idx2label,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"num_classes\": num_classes\n",
    "}\n",
    "\n",
    "with open(\"cbow_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
