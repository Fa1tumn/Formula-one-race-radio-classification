{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6cafd29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "733890c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Make a gap so you can weave on this straight and then we’ll have a lift and brake into 11.', 'We’ll need a 1.50 lift into turn 10.', 'Brake into 8. Easy through 8.', 'No full pedal from now.', 'I have understeer at turn 80 and oversteer at turn 80.', 'Everything is overheating.', 'I don’t like the feedback on the start. Yeah, it’s too deep.', 'Front’s still too cold. Just had a test.', 'This tyre sucks.', 'We are boxing for hard tyres.', 'Keep working those tyres.', 'Focusing on putting temperature on these tyres.', 'Medium tyres through 11 and 12.', 'Our tyres are on mode 3A.', 'box box box.', 'Plan 12. Plus 0.5.', 'We are plan B, target lap, both safety car windows are open.', 'We’ll box this lap.', 'Next tyres will be a hard ollie.', 'You have clear, clear now. The car ahead is Stroll in turn 2.', 'Wind is reducing. Wind is reduced 5 kph.', 'Everyone slow ahead.', 'Lando is over his grid box.', 'Isaac has DRS behind.', 'WooHoo, let’s go baby! Love that.', 'I am stupid. I switching off everything.', 'Simple as that. What do you think? Do you think that was a Smooth operator~', 'Yeah leave me alone, Ricky.', 'What a race, P13 to P3. That’s why you should never give up.', 'There is a chance we lose the dash, if that happens just keep driving.', 'Gears are ****, what’s going on?', 'It felt all under control, and then we had a brake by wire failure.', 'Gears are ****, what’s going on?', 'Gears are ****, what’s going on?', 'Turn 10, good improvement. Turn 4, we’re losing 40 milliseconds compared to Lewis.', 'Comparator on target paces and cars ahead, 37. med 37.', 'Turn 10, good improvement. Turn 4, we’re losing 40 milliseconds compared to Lewis.', 'Turn 10, good improvement. Turn 4, we’re losing 40 milliseconds compared to Lewis.', 'Turn 10, good improvement. Turn 4, we’re losing 40 milliseconds compared to Lewis.']\n",
      "['차량 조작 지시', '차량 조작 지시', '차량 조작 지시', '차량 조작 지시', '차량 상태 피드백', '차량 상태 피드백', '차량 상태 피드백', '차량 상태 피드백', '차량 상태 피드백', '타이어 관련', '타이어 관련', '타이어 관련', '타이어 관련', '타이어 관련', '전략 및 계획', '전략 및 계획', '전략 및 계획', '전략 및 계획', '전략 및 계획', '트랙/상대 정보', '트랙/상대 정보', '트랙/상대 정보', '트랙/상대 정보', '트랙/상대 정보', '감성/개인 표현', '감성/개인 표현', '감성/개인 표현', '감성/개인 표현', '감성/개인 표현', '시스템/하드웨어 문제', '시스템/하드웨어 문제', '시스템/하드웨어 문제', '시스템/하드웨어 문제', '시스템/하드웨어 문제', '데이터 피드백/비교', '데이터 피드백/비교', '데이터 피드백/비교', '데이터 피드백/비교', '데이터 피드백/비교']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "#remove the first row because it is 'nan'\n",
    "texts = data[\"Message\"].tolist()[1:]\n",
    "labels = data[\"Category\"].tolist()[1:]\n",
    "print(texts)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0c21b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n=2):\n",
    "    \"\"\"Generate n-grams from a given text.\"\"\"\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5167a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, ngram_range=(1, 2)):\n",
    "    \"\"\"Build vocabulary for n-grams.\"\"\"\n",
    "    vocab = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            ngrams = generate_ngrams(text, n)\n",
    "            for ngram in ngrams:\n",
    "                vocab[ngram] += 1\n",
    "    return {word: idx for idx, word in enumerate(vocab.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "34fe08c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_texts(texts, vocab, ngram_range=(1, 2)):\n",
    "    \"\"\"Convert texts to vectorized form using the vocabulary.\"\"\"\n",
    "    vectors = []\n",
    "    for text in texts:\n",
    "        vec = np.zeros(len(vocab))\n",
    "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "            ngrams = generate_ngrams(text, n)\n",
    "            for ngram in ngrams:\n",
    "                if ngram in vocab:\n",
    "                    vec[vocab[ngram]] += 1\n",
    "        vectors.append(vec)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "155431c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary and vectorize texts\n",
    "vocab = build_vocab(texts, ngram_range=(1, 3))\n",
    "X = vectorize_texts(texts, vocab, ngram_range=(1, 3))\n",
    "\n",
    "# Encode labels\n",
    "label2idx = {label: idx for idx, label in enumerate(set(labels))}\n",
    "y = np.array([label2idx[label] for label in labels])\n",
    "num_classes = len(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a6ae58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.3, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(len(X) * (1 - test_size))\n",
    "    train_idx, test_idx = indices[:split], indices[split:]\n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5d83df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(probs, y_true):\n",
    "    batch_size = probs.shape[0]\n",
    "    log_probs = np.log(probs + 1e-9)\n",
    "    loss = -log_probs[range(batch_size), y_true].mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e6f25d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "W = np.random.randn(input_dim, num_classes) * 0.01\n",
    "b = np.zeros((1, num_classes))\n",
    "learning_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f73d942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.0755\n",
      "Epoch 10000: Loss = 0.0139\n",
      "Epoch 20000: Loss = 0.0066\n",
      "Epoch 30000: Loss = 0.0044\n",
      "Epoch 40000: Loss = 0.0032\n",
      "Epoch 50000: Loss = 0.0026\n",
      "Epoch 60000: Loss = 0.0021\n",
      "Epoch 70000: Loss = 0.0018\n",
      "Epoch 80000: Loss = 0.0016\n",
      "Epoch 90000: Loss = 0.0014\n"
     ]
    }
   ],
   "source": [
    "epochs = 100000\n",
    "for epoch in range(epochs):\n",
    "    logits = np.dot(X_train, W) + b\n",
    "    probs = softmax(logits)\n",
    "    \n",
    "    loss = cross_entropy(probs, y_train)\n",
    "\n",
    "    # 수동 gradient 계산\n",
    "    N = X_train.shape[0]\n",
    "    one_hot = np.zeros_like(probs)\n",
    "    one_hot[np.arange(N), y_train] = 1\n",
    "    dL_dz = (probs - one_hot) / N  # softmax + cross entropy gradient\n",
    "\n",
    "    # Weight & bias gradients\n",
    "    dW = np.dot(X_train.T, dL_dz)\n",
    "    db = np.sum(dL_dz, axis=0, keepdims=True)\n",
    "\n",
    "    # 경사하강법 업데이트\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "64ababbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 정확도: 0.5\n"
     ]
    }
   ],
   "source": [
    "test_logits = np.dot(X_test, W) + b\n",
    "test_probs = softmax(test_logits)\n",
    "preds = np.argmax(test_probs, axis=1)\n",
    "acc = np.mean(preds == y_test)\n",
    "\n",
    "print(\"\\n테스트 정확도:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2991ca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"box box box\" | Predicted Label: \"전략 및 계획\"\n",
      "Text: \"My tyres are gone.\" | Predicted Label: \"타이어 관련\"\n",
      "Text: \"Tell him to get out of the way.\" | Predicted Label: \"차량 상태 피드백\"\n",
      "Text: \"We need to push for track position.\" | Predicted Label: \"타이어 관련\"\n"
     ]
    }
   ],
   "source": [
    "# 새로운 데이터 예측\n",
    "new_texts = [\n",
    "    \"box box box\", \n",
    "    \"My tyres are gone.\",\n",
    "    \"Tell him to get out of the way.\",\n",
    "    \"We need to push for track position.\",\n",
    "    \n",
    "]\n",
    "\n",
    "# 1. N-gram 벡터화\n",
    "new_X_data = []\n",
    "for text in new_texts:\n",
    "    vec = np.zeros(len(vocab))\n",
    "    for n in range(1, 3):  # n-gram 범위 (1, 2)\n",
    "        ngrams = generate_ngrams(text, n)\n",
    "        for ngram in ngrams:\n",
    "            if ngram in vocab:\n",
    "                vec[vocab[ngram]] += 1\n",
    "    new_X_data.append(vec)\n",
    "\n",
    "new_X = np.array(new_X_data)\n",
    "\n",
    "# 2. 예측\n",
    "new_logits = np.dot(new_X, W) + b\n",
    "new_probs = softmax(new_logits)\n",
    "new_y_pred = np.argmax(new_probs, axis=1)\n",
    "\n",
    "# 3. 예측 결과를 레이블로 변환\n",
    "idx2label = {v: k for k, v in label2idx.items()}\n",
    "new_y_pred_labels = [idx2label[idx] for idx in new_y_pred]\n",
    "\n",
    "# 4. 결과 출력\n",
    "for text, label in zip(new_texts, new_y_pred_labels):\n",
    "    print(f\"Text: \\\"{text}\\\" | Predicted Label: \\\"{label}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "765c173f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 성공적으로 저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 모델과 필요한 데이터 저장\n",
    "model_data = {\n",
    "    \"W\": W,  # 가중치 행렬\n",
    "    \"b\": b,  # 편향\n",
    "    \"vocab\": vocab,  # N-gram 단어 사전\n",
    "    \"label2idx\": label2idx,  # 레이블 -> 인덱스 매핑\n",
    "    \"idx2label\": {v: k for k, v in label2idx.items()},  # 인덱스 -> 레이블 매핑\n",
    "    \"ngram_range\": (1, 2)  # N-gram 범위\n",
    "}\n",
    "\n",
    "with open(\"ngram_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\"모델이 성공적으로 저장되었습니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
